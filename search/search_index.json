{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Readers Guide","text":"<p>The sole purpose of this documentation is to ensure the approaches used in tuning one of the Spark ETL(Extract,  Transform, Load) jobs and share it with a broader audience who could benefit from it and most importantly receive feedback. The  findings are by no means set guidelines as they are limited to address one of the performance issue we faced.</p>"},{"location":"#the-need-for-tuning-spark-jobs","title":"The need for tuning Spark jobs?","text":"<p>We have all heard the term \"Data is King.\" As more and more organizations realize the power of  data, big-data and analytics has played a vital role in their organization's digital transformation.  As the need for most companies to transition their legacy data from traditional in-house  to the cloud raises, cloud platforms such as AWS, Microsoft Azure, and Google cloud are becoming more prominent  in the organization's data-driven journey.  While these platforms provide on-demand, pay-as-you-go, and set guidelines to help reduce the overall cost, organizations are still required to optimize their usage. </p>"},{"location":"#additional-comments","title":"Additional comments","text":"<p>This documentation uses Mkdocs, a static site generator primarily used for project documentation  configured using YAML.</p> <p>Use the toggle option next to the search bar to switch between dark and light mode</p>"},{"location":"aboutme/","title":"Sanath Patil","text":"<p>A data professional with nearly two years of professional experience as a software engineer and around two years  of experience in data science and data engineering. Presently interning at Rocket Central- a Detroit-based technology company that supports the diverse  business needs across the Rocket Companies- as a data engineer helping build, test, and deploy  scalable ETL pipelines. </p> <p>A graduate student pursuing Master's degree in Business Analytics at The University of Texas at  Dallas graduating in May 2023. Under the realm of data science and engineering, my passion  is curating and pre-processing data to gather vital information that bridges the business  understanding to requirements that drives strategic decisions which bring value.</p> <p>How to reach out?</p> <p>GitHub and LinkedIn links are on the bottom right!</p>"},{"location":"pyspark/","title":"Performance Enhancement","text":""},{"location":"pyspark/#overview","title":"Overview","text":"<p>One of the ETLs in production was paused due to a performance issue. This presented an opportunity to work on fine-tuning the performance. Numerous approaches  in this documentation is discussed without giving out the intricacies of the ETL.</p>"},{"location":"pyspark/#etl-architecture","title":"ETL Architecture","text":""},{"location":"pyspark/#approaches","title":"Approaches","text":"<p>Though multiple approaches were tried, we will discuss the best practices that fit the need. The methods discussed are more relevant to the problem at hand, as every scenario is different.  It's important to know that choosing an approach based on the overall architecture will provide the best results.</p>"},{"location":"pyspark/#repartitioning-target-table-pyspark-doc-repartition","title":"Repartitioning Target table: PySpark Doc-repartition","text":"<p>As discussed in the architecture, the target table which is read for performing some transformations by joining with the source data frames, repartitioning the target table showed significant improvement in the overall execution time.  Although Spark repartition is considered to be expensive as it involves shuffling of data, this  was implemented as it was believed to ease the join condition in the long run. <pre><code>df.repartition(numPartitions, *cols)\n</code></pre></p> <p>Suggessted optimal alternative- coalesce()  An alternative approach if there is a need for repartitioning the dataframe with minimal shuffling of data. <pre><code>df.coalesce(numPartitions)\n</code></pre> PySpark Doc-coalesce</p> <p>It's important to keep in mind that repartitioning dataframes on unique columns such at date-key may result in a small file problem </p>"},{"location":"pyspark/#broadcast-join-pyspark-broadcast-join","title":"Broadcast join: PySpark Broadcast join","text":"<p>The source data frames(smaller than the target table) had to be joined to the newly partitioned target table for  some transformations. Since this resulted in a small data frame - large data frame scenario, the immediate approach was  to broadcast the smaller dataframe- as this may reduce data shuffling.</p> <p>Although on paper this approach seemed a feasible option, it turned out not to be viable since no significant boost in terms of performance was observed.</p>"},{"location":"pyspark/#cache-or-persist","title":"Cache or Persist:","text":"<p>This approach showed significant improvement in performance and complemented the repartitioning of the target table. After extracting the source data frames and joining them,  numerous transformations were in place which increased the logical query plan for execution. Therefore, after analyzing the logical plan of the data frame, as the sources were used  repeatedly, caching the dataframe before performing additional transformations reduced the logical plan considerably.</p> <p><pre><code>df.explain(exteded=False)\nNote: extended can be used to get the physical plan\n</code></pre> PySpark df.explain()</p> <p>This article highlights how Spark leverages caching- Towards Data Science</p> <p>It's important to know that caching the data frame is still a cost and hence may result in failures due to out of memory exception if used for more extensive data frames.</p> <p>Note: Since Spark uses lazy evaluation, in order to leverage the optimization techniques for a transformation, an action needs to be performed- such as collect(), count() etc</p>"},{"location":"pyspark/#withcolumn-vs-select","title":"withColumn() v/s select():","text":"<p>withColumn returns a new Data Frame, used for adding or replacing an existing column. Using this method iteratively to add or modify multiple columns may result in substantial query plans  which reduces overall performance. Instead, a better approach would be to use the select statement, even for multiple columns, which still returns a new data frame.  Although on fewer columns, it is challenging to measure the difference in performance, this would still be an optimal approach in the long run. PySpark withColumn()</p> <ul> <li>Captures difference between the two-stackoverflow</li> </ul>"},{"location":"pyspark/#conclusion","title":"Conclusion","text":"<p>The approaches discussed above are some optimization techniques that best suited the problem at hand. A key strategy that will increase performance is figuring out- how to reduce data shuffle?</p> <p>More guidelines are provided in the official Spark documentation- Performance Tuning. As discussed earlier, performance tuning is a huge challenge, and understanding the data, architecture, and business goals are some  nuances that will help you choose optimal approaches.</p>"}]}